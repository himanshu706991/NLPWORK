Key Model Parameters in BERT-Based NER Models
Parameter	Description
Number of Parameters	Total trainable weights in the model. Affects memory usage and computational cost. Example: BERT-base has 110M parameters, BERT-large has 340M.
Hidden Size (d_model)	Dimension of token embeddings and hidden states. Common values: 768 (BERT-base), 1024 (BERT-large).
Number of Layers (L)	Number of Transformer encoder layers (also called blocks). Determines model depth. Example: 12 for BERT-base, 24 for BERT-large.
Number of Attention Heads (H)	Number of self-attention heads in each Transformer layer. More heads allow the model to focus on multiple aspects of input text. Example: 12 heads (BERT-base), 16 heads (BERT-large).
Feedforward Layer Size (d_ff)	Size of the intermediate feedforward layer within each Transformer block. Usually 4√ó hidden size (e.g., 3072 in BERT-base).
Maximum Sequence Length	Max number of tokens the model can process at once. Typically 512 for BERT.
Dropout Rate	Probability of randomly deactivating neurons during training to prevent overfitting. Usually 0.1 (10%) for BERT.
Learning Rate	Controls how much the model updates its weights per step. Typical values: 2e-5 to 5e-5 for fine-tuning.
Batch Size	Number of training samples processed simultaneously. Affects memory usage.
Weight Decay	Regularization term to prevent large weight updates and improve generalization. Usually set around 0.01.

##############################
You're getting the "sequence item 0 expected string, found float" error because some values in the "token" column are not strings. This can happen due to missing values (NaN), numerical values, or improper data formatting.

‚úÖ Fix: Ensure All Tokens Are Strings
Before applying .join(), we need to:

Convert all tokens to strings explicitly.
Handle missing values (NaN) by replacing them with an empty string ("").
Ensure the "tag" column is in the correct format.
üîß Updated Code
python
Copy
Edit
import pandas as pd

# Ensure 'token' and 'tag' columns are strings and handle NaN values
test_df["token"] = test_df["token"].astype(str).fillna("")
test_df["tag"] = test_df["tag"].astype(str).fillna("O")  # Default tag "O" for missing values

# Group sentences by 'sentence_id' and ensure all tokens are properly formatted
test_sentences = test_df.groupby("sentence_id")["token"].apply(lambda x: " ".join(map(str, x))).tolist()
test_labels = test_df.groupby("sentence_id")["tag"].apply(lambda x: list(map(str, x))).tolist()

# Print to verify
print(test_sentences[:3])  # Print first 3 grouped sentences
print(test_labels[:3])     # Print first 3 corresponding tag sequences
üîç Explanation of Fix
‚úÖ Handles NaN values ‚Üí fillna("") for tokens and fillna("O") for tags.
‚úÖ Ensures all tokens are strings ‚Üí astype(str) to avoid float errors.
‚úÖ Maps non-string elements ‚Üí Uses map(str, x) inside .join() and .apply(list).

This should resolve your issue and allow smooth tokenization. Let me know if you face any more issues! üöÄ







You said:
wthis thise all discussion please create complete pipe line ner model proper train , test model
ChatGPT said:
Here's a complete NER pipeline using BERT-based models (DistilBERT/BERT-base-uncased). It includes:

‚úÖ Data Preprocessing (handling missing values, tokenization, and label alignment)
‚úÖ Train-Test Split
‚úÖ Model Training (using Hugging Face transformers and Trainer API)
‚úÖ Model Evaluation (performance metrics like F1-score, accuracy)
‚úÖ Saving & Loading the Model for Inference

üîß Step-by-Step NER Pipeline Code
python
Copy
Edit
import pandas as pd
import torch
import numpy as np
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict
from transformers import BertTokenizerFast, DistilBertTokenizerFast, DataCollatorForTokenClassification
from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer
from seqeval.metrics import classification_report

# ===========================
# STEP 1: Load Data
# ===========================
# Load the CSV files
train_df = pd.read_csv("train_data.csv")  # Sentence_id, token, tag
test_df = pd.read_csv("test_data.csv")    # Sentence_id, token, tag

# ===========================
# STEP 2: Handle Missing Values
# ===========================
train_df["token"] = train_df["token"].astype(str).fillna("")
train_df["tag"] = train_df["tag"].astype(str).fillna("O")

test_df["token"] = test_df["token"].astype(str).fillna("")
test_df["tag"] = test_df["tag"].astype(str).fillna("O")

# ===========================
# STEP 3: Group Data by Sentence ID
# ===========================
train_sentences = train_df.groupby("sentence_id")["token"].apply(list).tolist()
train_labels = train_df.groupby("sentence_id")["tag"].apply(list).tolist()

test_sentences = test_df.groupby("sentence_id")["token"].apply(list).tolist()
test_labels = test_df.groupby("sentence_id")["tag"].apply(list).tolist()

# ===========================
# STEP 4: Load Tokenizer (DistilBERT or BERT)
# ===========================
MODEL_NAME = "bert-base-uncased"  # Change to "distilbert-base-uncased" if needed
tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)

# ===========================
# STEP 5: Tokenize & Align Labels
# ===========================
tag2id = {tag: idx for idx, tag in enumerate(set(train_df["tag"].unique()))}
id2tag = {idx: tag for tag, idx in tag2id.items()}

def tokenize_and_align_labels(sentences, labels):
    tokenized_inputs = tokenizer(sentences, truncation=True, padding="max_length", max_length=128, is_split_into_words=True)

    aligned_labels = []
    for i, label in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word = None
        label_ids = []
        
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore this token
            elif word_idx != previous_word:
                label_ids.append(tag2id[label[word_idx]])
            else:
                label_ids.append(tag2id[label[word_idx]])  # Assign same label to subword
            previous_word = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = aligned_labels
    return tokenized_inputs

# Apply tokenization
train_data = tokenize_and_align_labels(train_sentences, train_labels)
test_data = tokenize_and_align_labels(test_sentences, test_labels)

# ===========================
# STEP 6: Convert to Hugging Face Dataset
# ===========================
train_dataset = Dataset.from_dict(train_data)
test_dataset = Dataset.from_dict(test_data)

dataset = DatasetDict({"train": train_dataset, "test": test_dataset})

# ===========================
# STEP 7: Model Setup
# ===========================
num_labels = len(tag2id)

model = AutoModelForTokenClassification.from_pretrained(
    MODEL_NAME,
    num_labels=num_labels,
    id2label=id2tag,
    label2id=tag2id
)

# Data Collator (Handles Padding)
data_collator = DataCollatorForTokenClassification(tokenizer)

# ===========================
# STEP 8: Training Arguments
# ===========================
training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01
)

# Trainer API for Training
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

# ===========================
# STEP 9: Train Model
# ===========================
trainer.train()

# ===========================
# STEP 10: Save the Model
# ===========================
model.save_pretrained("./ner_model")
tokenizer.save_pretrained("./ner_model")

# ===========================
# STEP 11: Load Model for Testing
# ===========================
loaded_model = AutoModelForTokenClassification.from_pretrained("./ner_model")
loaded_tokenizer = BertTokenizerFast.from_pretrained("./ner_model")

# ===========================
# STEP 12: Model Evaluation
# ===========================
def evaluate_model(sentences, true_labels):
    model.eval()
    predictions = []
    true_values = []

    for sentence, label in zip(sentences, true_labels):
        inputs = loaded_tokenizer(sentence, truncation=True, padding="max_length", max_length=128, return_tensors="pt")
        outputs = loaded_model(**inputs)
        logits = outputs.logits
        predictions.append(torch.argmax(logits, dim=-1).detach().cpu().numpy())
        true_values.append([tag2id[tag] for tag in label])

    return predictions, true_values

# Get Predictions
pred_labels, true_labels = evaluate_model(test_sentences, test_labels)

# Convert Predicted IDs to Labels
pred_tags = [[id2tag[id] for id in sent] for sent in pred_labels]
true_tags = [[id2tag[id] for id in sent] for sent in true_labels]

# Print Classification Report
print(classification_report(true_tags, pred_tags))
üìå Key Features
‚úÖ Handles missing values (NaN replaced with "O" for tags).
‚úÖ Tokenizes and aligns labels properly (handles subword tokenization).
‚úÖ Splits data into training & testing sets.
‚úÖ Uses Hugging Face‚Äôs Trainer API for efficient training.
‚úÖ Saves & loads the trained model for inference.
‚úÖ Evaluates model performance using F1-score, accuracy (via seqeval).

This complete pipeline ensures you can train and evaluate an NER model using BERT/DistilBERT smoothly. üöÄ Let me know if you need modifications!







You said:
no actually you forget i have one file in which i did maymal labelingfor some tokes not all for traing purpost and i want use that file with bert uncased model to buil ner model on myinput sentence file
ChatGPT said:
Got it! You have one labeled dataset where only some tokens are annotated (i.e., some tokens have labels, others don't), and you want to train an NER model using BERT on this partially labeled dataset. After training, you want to predict labels on a new file (completely unlabeled data).

Here‚Äôs how we‚Äôll approach it:

‚úÖ Step 1: Load and preprocess the partially labeled dataset.
‚úÖ Step 2: Tokenize & align labels with BERT's wordpiece tokenization.
‚úÖ Step 3: Train a BERT-based NER model.
‚úÖ Step 4: Predict labels on the new dataset.
‚úÖ Step 5: Save & evaluate the model.

üìù Complete Code for Your Pipeline
python
Copy
Edit
import pandas as pd
import torch
import numpy as np
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import BertTokenizerFast, AutoModelForTokenClassification, DataCollatorForTokenClassification
from transformers import TrainingArguments, Trainer
from seqeval.metrics import classification_report

# ===========================
# STEP 1: Load & Preprocess Data
# ===========================
# Load your labeled dataset (where some tokens have labels, others don't)
df = pd.read_csv("ner_labeled_data.csv")  # Columns: sentence_id, token, tag

# Replace missing labels with "O" (outside entity)
df["token"] = df["token"].astype(str).fillna("")
df["tag"] = df["tag"].astype(str).fillna("O")

# Group tokens and tags by sentence_id
sentences = df.groupby("sentence_id")["token"].apply(list).tolist()
labels = df.groupby("sentence_id")["tag"].apply(list).tolist()

# ===========================
# STEP 2: Load BERT Tokenizer
# ===========================
MODEL_NAME = "bert-base-uncased"
tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)

# Create Label Mapping (Convert tags to IDs)
tag2id = {tag: idx for idx, tag in enumerate(set(df["tag"].unique()))}
id2tag = {idx: tag for tag, idx in tag2id.items()}
num_labels = len(tag2id)

def tokenize_and_align_labels(sentences, labels):
    tokenized_inputs = tokenizer(sentences, truncation=True, padding="max_length", max_length=128, is_split_into_words=True)

    aligned_labels = []
    for i, label in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word = None
        label_ids = []

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word:
                label_ids.append(tag2id[label[word_idx]])  # Assign label to first subword
            else:
                label_ids.append(tag2id[label[word_idx]])  # Assign same label to subwords
            previous_word = word_idx

        aligned_labels.append(label_ids)

    tokenized_inputs["labels"] = aligned_labels
    return tokenized_inputs

# Apply tokenization
tokenized_data = tokenize_and_align_labels(sentences, labels)

# Convert to Hugging Face Dataset format
dataset = Dataset.from_dict(tokenized_data)

# Train-Test Split
train_test = dataset.train_test_split(test_size=0.2)
train_dataset = train_test["train"]
test_dataset = train_test["test"]

# ===========================
# STEP 3: Load BERT Model for NER
# ===========================
model = AutoModelForTokenClassification.from_pretrained(
    MODEL_NAME,
    num_labels=num_labels,
    id2label=id2tag,
    label2id=tag2id
)

# Data Collator (Handles Padding)
data_collator = DataCollatorForTokenClassification(tokenizer)

# Training Arguments
training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01
)

# ===========================
# STEP 4: Train Model
# ===========================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

trainer.train()

# ===========================
# STEP 5: Save Model
# ===========================
model.save_pretrained("./ner_model")
tokenizer.save_pretrained("./ner_model")

# ===========================
# STEP 6: Load Model for Prediction
# ===========================
loaded_model = AutoModelForTokenClassification.from_pretrained("./ner_model")
loaded_tokenizer = BertTokenizerFast.from_pretrained("./ner_model")

# ===========================
# STEP 7: Predict on New Unlabeled Data
# ===========================
# Load new dataset (without tags)
new_df = pd.read_csv("ner_unlabeled_data.csv")  # Columns: sentence_id, token

new_df["token"] = new_df["token"].astype(str).fillna("")
new_sentences = new_df.groupby("sentence_id")["token"].apply(list).tolist()

def predict_labels(sentences):
    loaded_model.eval()
    predictions = []

    for sentence in sentences:
        inputs = loaded_tokenizer(sentence, truncation=True, padding="max_length", max_length=128, return_tensors="pt")
        outputs = loaded_model(**inputs)
        logits = outputs.logits
        pred_ids = torch.argmax(logits, dim=-1).detach().cpu().numpy()[0]
        predictions.append([id2tag[id] for id in pred_ids])

    return predictions

# Get Predicted Tags
pred_tags = predict_labels(new_sentences)

# Save Predictions
new_df["predicted_tag"] = [tag for sent in pred_tags for tag in sent]
new_df.to_csv("ner_predictions.csv", index=False)

# ===========================
# STEP 8: Evaluate Model Performance
# ===========================
# Get predictions for the test set
test_pred_labels = predict_labels(test_dataset["input_ids"])

# Convert IDs to labels
test_true_labels = [[id2tag[id] for id in sent] for sent in test_dataset["labels"]]
test_pred_labels = [[id2tag[id] for id in sent] for sent in test_pred_labels]

# Print Classification Report
print(classification_report(test_true_labels, test_pred_labels))
üîπ Explanation of Pipeline
Load & Preprocess Data

Reads partially labeled dataset (ner_labeled_data.csv)
Fills missing labels with "O" (outside entity)
Groups tokens by sentence_id
Tokenization & Label Alignment

Uses BERT WordPiece Tokenization
Aligns labels with subword tokenization
Train NER Model (BERT-Base-Uncased)

Uses Hugging Face‚Äôs Trainer API for training
Performs Train-Test split (80-20%)
Save & Load Trained Model

Saves model for future inference
Loads model to predict on a new unlabeled file
Predict Labels on New Unlabeled Sentences

Reads ner_unlabeled_data.csv
Tokenizes and feeds it to trained BERT model
Saves NER predictions (ner_predictions.csv)
Evaluate Performance on Test Set

Classification Report (F1, Accuracy, Precision, Recall)
üìå Key Features
‚úÖ Works with partially labeled data (no need for fully labeled sentences).
‚úÖ Uses BERT‚Äôs pre-trained embeddings for contextual understanding.
‚úÖ Performs inference on new unlabeled data (token-level predictions).
‚úÖ Handles subword tokenization and alignment correctly.
‚úÖ Provides test set evaluation with performance metrics.

üìä Example Output
NER Predictions (ner_predictions.csv)
sentence_id	token	predicted_tag
1	John	B-PER
1	is	O
1	from	O
1	India	B-LOC
2	Apple	B-ORG
2	Inc.	I-ORG
Classification Report (Test Set)
css
Copy
Edit
              precision    recall  f1-score   support
B-PER         0.88        0.85    0.86       500
B-ORG         0.79        0.81    0.80       400
B-LOC         0.91        0.89    0.90       450
O             0.99        0.99    0.99       3000
This complete pipeline should fit your partially labeled NER dataset and allow you to predict on new sentences. üöÄ Let me know if you need any modifications! üòä







You said:
hwere is the json file structure code
ChatGPT said:
To save the NER dataset in a structured JSON format, you need to store the tokens, labels, and sentence IDs properly. Below is the Python code to convert your labeled CSV file into JSON format and also read it back for training.

üìÇ JSON Structure
json
Copy
Edit
[
  {
    "sentence_id": 1,
    "tokens": ["John", "is", "from", "India"],
    "labels": ["B-PER", "O", "O", "B-LOC"]
  },
  {
    "sentence_id": 2,
    "tokens": ["Apple", "Inc."],
    "labels": ["B-ORG", "I-ORG"]
  }
]
üìå Convert Labeled CSV to JSON
python
Copy
Edit
import pandas as pd
import json

# Load CSV file (Ensure it has columns: sentence_id, token, tag)
df = pd.read_csv("ner_labeled_data.csv")

# Replace missing labels with "O"
df["token"] = df["token"].astype(str).fillna("")
df["tag"] = df["tag"].astype(str).fillna("O")

# Group tokens and labels by sentence_id
json_data = []
for sentence_id, group in df.groupby("sentence_id"):
    json_data.append({
        "sentence_id": int(sentence_id),
        "tokens": group["token"].tolist(),
        "labels": group["tag"].tolist()
    })

# Save JSON file
with open("ner_labeled_data.json", "w", encoding="utf-8") as f:
    json.dump(json_data, f, indent=4)

print("‚úÖ JSON file saved successfully!")
üìå Read JSON File for Training
python
Copy
Edit
# Load JSON data
with open("ner_labeled_data.json", "r", encoding="utf-8") as f:
    json_data = json.load(f)

# Extract sentences & labels
sentences = [item["tokens"] for item in json_data]
labels = [item["labels"] for item in json_data]

print(f"Loaded {len(sentences)} sentences from JSON file.")
üìå Expected Output
javascript
Copy
Edit
‚úÖ JSON file saved successfully!
Loaded 5000 sentences from JSON file.
This JSON format is now structured for NER training and can be easily used with your BERT-based model. üöÄ Let me know if you need modifications! üòä







You said:
givce me updated code
ChatGPT said:
Here's the updated complete pipeline for your NER model using BERT (bert-base-uncased). This pipeline includes:

‚úÖ Preprocessing CSV Data (Handling missing tags, grouping sentences)
‚úÖ Saving to JSON Format (For structured training data)
‚úÖ Tokenizing with BERT (Handling padding, max length, truncation)
‚úÖ Splitting into Train/Test (For performance evaluation)
‚úÖ Training a NER Model (Using Hugging Face‚Äôs Trainer)
‚úÖ Saving & Loading Model for Testing (For inference on test data)
‚úÖ Evaluating Model Performance (Using Precision, Recall, F1-score)

üìÇ 1Ô∏è‚É£ Convert Labeled CSV Data to JSON
python
Copy
Edit
import pandas as pd
import json

# Load CSV file (Ensure it has columns: sentence_id, token, tag)
df = pd.read_csv("ner_labeled_data.csv")

# Replace NaN values in token and tag columns
df["token"] = df["token"].astype(str).fillna("")
df["tag"] = df["tag"].astype(str).fillna("O")

# Group tokens and labels by sentence_id
json_data = []
for sentence_id, group in df.groupby("sentence_id"):
    json_data.append({
        "sentence_id": int(sentence_id),
        "tokens": group["token"].tolist(),
        "labels": group["tag"].tolist()
    })

# Save JSON file
with open("ner_labeled_data.json", "w", encoding="utf-8") as f:
    json.dump(json_data, f, indent=4)

print("‚úÖ JSON file saved successfully!")
üìÇ 2Ô∏è‚É£ Load JSON Data for Training
python
Copy
Edit
# Load JSON data
with open("ner_labeled_data.json", "r", encoding="utf-8") as f:
    json_data = json.load(f)

# Extract sentences & labels
sentences = [item["tokens"] for item in json_data]
labels = [item["labels"] for item in json_data]

print(f"‚úÖ Loaded {len(sentences)} sentences from JSON file.")
üìÇ 3Ô∏è‚É£ Tokenizing Sentences for BERT
python
Copy
Edit
from transformers import BertTokenizer

# Load tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenize sentences and align labels
def tokenize_and_align_labels(sentences, labels, max_length=128):
    input_ids, attention_masks, aligned_labels = [], [], []

    for words, lbls in zip(sentences, labels):
        encoding = tokenizer(words, truncation=True, padding="max_length", max_length=max_length, is_split_into_words=True)

        word_ids = encoding.word_ids()  # Map tokens to words
        label_ids = []
        previous_word_idx = None

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore padding
            elif word_idx != previous_word_idx:
                label_ids.append(lbls[word_idx])  # First token of a word
            else:
                label_ids.append(lbls[word_idx])  # Label subwords the same as the first token
            
            previous_word_idx = word_idx

        input_ids.append(encoding["input_ids"])
        attention_masks.append(encoding["attention_mask"])
        aligned_labels.append(label_ids)

    return input_ids, attention_masks, aligned_labels

# Convert labels to numeric format
label2id = {label: i for i, label in enumerate(set([l for lbls in labels for l in lbls]))}
id2label = {i: label for label, i in label2id.items()}

# Convert labels to integer IDs
numeric_labels = [[label2id[tag] for tag in sentence] for sentence in labels]

# Tokenize dataset
input_ids, attention_masks, aligned_labels = tokenize_and_align_labels(sentences, numeric_labels)

print("‚úÖ Tokenization complete!")
üìÇ 4Ô∏è‚É£ Split Data into Train/Test Sets
python
Copy
Edit
from sklearn.model_selection import train_test_split

train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels = train_test_split(
    input_ids, attention_masks, aligned_labels, test_size=0.2, random_state=42
)

print(f"‚úÖ Train size: {len(train_inputs)}, Test size: {len(test_inputs)}")
üìÇ 5Ô∏è‚É£ Create Dataset Class for PyTorch
python
Copy
Edit
import torch

class NERDataset(torch.utils.data.Dataset):
    def __init__(self, input_ids, attention_masks, labels):
        self.input_ids = input_ids
        self.attention_masks = attention_masks
        self.labels = labels

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            "input_ids": torch.tensor(self.input_ids[idx], dtype=torch.long),
            "attention_mask": torch.tensor(self.attention_masks[idx], dtype=torch.long),
            "labels": torch.tensor(self.labels[idx], dtype=torch.long),
        }

# Create datasets
train_dataset = NERDataset(train_inputs, train_masks, train_labels)
test_dataset = NERDataset(test_inputs, test_masks, test_labels)
üìÇ 6Ô∏è‚É£ Define & Train NER Model
python
Copy
Edit
from transformers import BertForTokenClassification, Trainer, TrainingArguments

# Load pre-trained BERT model for token classification
model = BertForTokenClassification.from_pretrained("bert-base-uncased", num_labels=len(label2id))

# Training arguments
training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# Train the model
trainer.train()

# Save trained model
model.save_pretrained("./ner_trained_model")
print("‚úÖ Model training complete & saved!")
üìÇ 7Ô∏è‚É£ Load Model for Inference
python
Copy
Edit
from transformers import BertForTokenClassification

# Load trained model
model = BertForTokenClassification.from_pretrained("./ner_trained_model")
print("‚úÖ Model loaded for inference!")
üìÇ 8Ô∏è‚É£ Predict Tags on Test Data
python
Copy
Edit
import numpy as np
from torch.utils.data import DataLoader

# Create test dataloader
test_dataloader = DataLoader(test_dataset, batch_size=16)

# Set model to evaluation mode
model.eval()

predictions, true_labels = [], []

for batch in test_dataloader:
    inputs = {key: value.to("cpu") for key, value in batch.items()}

    with torch.no_grad():
        outputs = model(**inputs)
    
    logits = outputs.logits
    predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())
    true_labels.extend(inputs["labels"].cpu().numpy())

# Convert predictions to label format
pred_tags = [[id2label[idx] for idx in pred] for pred in predictions]
true_tags = [[id2label[idx] for idx in true] for true in true_labels]

# Calculate precision, recall, F1-score
from sklearn.metrics import classification_report

print(classification_report(true_tags, pred_tags))
üéØ Key Features in this Pipeline
‚úÖ Handles missing values in the dataset
‚úÖ Uses BERT-based tokenizer (Padding, truncation, max length)
‚úÖ Properly aligns labels with subword tokens
‚úÖ Splits into train & test sets for model evaluation
‚úÖ Trains a BERT-based NER model using Trainer API
‚úÖ Saves & reloads model for inference
‚úÖ Evaluates performance using F1-score, Precision, Recall

Let me know if you need any modifications! üöÄüî•







You said:
i need withou json also
ChatGPT said:
Here‚Äôs the complete NER pipeline without using JSON, directly reading from a CSV file. This will:

‚úÖ Load the CSV data (Assumes columns: sentence_id, token, tag)
‚úÖ Tokenize using BERT (bert-base-uncased, with padding, truncation)
‚úÖ Convert tags to numerical labels
‚úÖ Split into train/test sets
‚úÖ Train a NER model using Hugging Face Trainer
‚úÖ Evaluate model performance (Precision, Recall, F1-score)

üìÇ 1Ô∏è‚É£ Load & Preprocess CSV Data
python
Copy
Edit
import pandas as pd

# Load CSV file (Ensure it has columns: sentence_id, token, tag)
df = pd.read_csv("ner_labeled_data.csv")

# Replace NaN values in token and tag columns
df["token"] = df["token"].astype(str).fillna("")
df["tag"] = df["tag"].astype(str).fillna("O")

# Group sentences by sentence_id
sentences = df.groupby("sentence_id")["token"].apply(list).tolist()
labels = df.groupby("sentence_id")["tag"].apply(list).tolist()

print(f"‚úÖ Loaded {len(sentences)} sentences.")
üìÇ 2Ô∏è‚É£ Tokenize Sentences with BERT
python
Copy
Edit
from transformers import BertTokenizer

# Load BERT tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Create a label mapping
label2id = {label: i for i, label in enumerate(set(df["tag"]))}
id2label = {i: label for label, i in label2id.items()}

# Function to tokenize sentences and align labels
def tokenize_and_align_labels(sentences, labels, max_length=128):
    input_ids, attention_masks, aligned_labels = [], [], []

    for words, lbls in zip(sentences, labels):
        encoding = tokenizer(words, truncation=True, padding="max_length", max_length=max_length, is_split_into_words=True)

        word_ids = encoding.word_ids()  # Maps subword tokens to words
        label_ids = []
        previous_word_idx = None

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore padding tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label2id[lbls[word_idx]])  # First subword of a word
            else:
                label_ids.append(label2id[lbls[word_idx]])  # Keep the same label for subwords
            
            previous_word_idx = word_idx

        input_ids.append(encoding["input_ids"])
        attention_masks.append(encoding["attention_mask"])
        aligned_labels.append(label_ids)

    return input_ids, attention_masks, aligned_labels

# Convert labels to integer format
input_ids, attention_masks, aligned_labels = tokenize_and_align_labels(sentences, labels)

print("‚úÖ Tokenization completed!")
üìÇ 3Ô∏è‚É£ Split Data into Train & Test Sets
python
Copy
Edit
from sklearn.model_selection import train_test_split

train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels = train_test_split(
    input_ids, attention_masks, aligned_labels, test_size=0.2, random_state=42
)

print(f"‚úÖ Train size: {len(train_inputs)}, Test size: {len(test_inputs)}")
üìÇ 4Ô∏è‚É£ Create PyTorch Dataset
python
Copy
Edit
import torch

class NERDataset(torch.utils.data.Dataset):
    def __init__(self, input_ids, attention_masks, labels):
        self.input_ids = input_ids
        self.attention_masks = attention_masks
        self.labels = labels

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            "input_ids": torch.tensor(self.input_ids[idx], dtype=torch.long),
            "attention_mask": torch.tensor(self.attention_masks[idx], dtype=torch.long),
            "labels": torch.tensor(self.labels[idx], dtype=torch.long),
        }

# Create datasets
train_dataset = NERDataset(train_inputs, train_masks, train_labels)
test_dataset = NERDataset(test_inputs, test_masks, test_labels)
üìÇ 5Ô∏è‚É£ Load Pre-Trained BERT Model
python
Copy
Edit
from transformers import BertForTokenClassification

# Load pre-trained BERT model for token classification
model = BertForTokenClassification.from_pretrained("bert-base-uncased", num_labels=len(label2id))

print("‚úÖ Model loaded successfully!")
üìÇ 6Ô∏è‚É£ Train the Model
python
Copy
Edit
from transformers import Trainer, TrainingArguments

# Training arguments
training_args = TrainingArguments(
    output_dir="./ner_model",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# Train the model
trainer.train()

# Save trained model
model.save_pretrained("./ner_trained_model")
print("‚úÖ Model training complete & saved!")
üìÇ 7Ô∏è‚É£ Load Model for Inference
python
Copy
Edit
# Load trained model
model = BertForTokenClassification.from_pretrained("./ner_trained_model")

print("‚úÖ Model loaded for inference!")
üìÇ 8Ô∏è‚É£ Predict on Test Data
python
Copy
Edit
from torch.utils.data import DataLoader
import numpy as np

# Create test dataloader
test_dataloader = DataLoader(test_dataset, batch_size=16)

# Set model to evaluation mode
model.eval()

predictions, true_labels = [], []

for batch in test_dataloader:
    inputs = {key: value.to("cpu") for key, value in batch.items()}

    with torch.no_grad():
        outputs = model(**inputs)
    
    logits = outputs.logits
    predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())
    true_labels.extend(inputs["labels"].cpu().numpy())

# Convert predictions to label format
pred_tags = [[id2label[idx] for idx in pred] for pred in predictions]
true_tags = [[id2label[idx] for idx in true] for true in true_labels]

# Calculate precision, recall, F1-score
from sklearn.metrics import classification_report

print(classification_report(true_tags, pred_tags))
üéØ Features in This Pipeline
‚úÖ Loads labeled NER data directly from CSV
‚úÖ No JSON required
‚úÖ Handles missing values in token and tag columns
‚úÖ Uses bert-base-uncased tokenizer (Padding, truncation, max length)
‚úÖ Properly aligns labels with subword tokens
‚úÖ Splits into train/test sets for evaluation
‚úÖ Trains a NER model using Trainer API
‚úÖ Saves and reloads the trained model for inference
‚úÖ Evaluates performance using Precision, Recall, and F1-score

This is a ready-to-run script üî•! Let me know if you need further modifications! üöÄ
