import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer

# Load tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Step 1: Load Data (Replace with your file path)
df = pd.read_csv("ner_dataset.csv")

# Step 2: Fill NaN labels with "O" (Outside tag for unlabeled tokens)
df["tag"] = df["tag"].fillna("O")

# Step 3: Group tokens and labels by sentence
sentence_tokens = df.groupby("sentence_id")["token"].apply(list).tolist()
sentence_labels = df.groupby("sentence_id")["tag"].apply(list).tolist()

# Step 4: Train-Test Split
train_tokens, test_tokens, train_labels, test_labels = train_test_split(
    sentence_tokens, sentence_labels, test_size=0.2, random_state=42
)

# Step 5: Tokenization with max_length, padding, and truncation
train_encodings = tokenizer(
    train_tokens, 
    is_split_into_words=True, 
    padding="max_length", 
    truncation=True, 
    max_length=128, 
    return_tensors="pt"
)

test_encodings = tokenizer(
    test_tokens, 
    is_split_into_words=True, 
    padding="max_length", 
    truncation=True, 
    max_length=128, 
    return_tensors="pt"
)

# Check Output
print("Example Train Token:", train_tokens[0])
print("Example Train Label:", train_labels[0])
print("Encoded Train Data:", train_encodings)
#####
Here's a complete pipeline for training a BERT-based NER model from scratch, using your labeled dataset with missing labels (110K tokens, with 10K labeled).

This pipeline includes:
‚úÖ Data Preparation (handling NaNs, train-test split)
‚úÖ Tokenization (subword token alignment, max_length, padding, truncation)
‚úÖ Model Training (BERT-based sequence classification with PyTorch)
‚úÖ Evaluation (on test set with precision, recall, F1-score)
‚úÖ Prediction (running the trained model on new test data)

1Ô∏è‚É£ Install Dependencies
bash
Copy
Edit
pip install transformers datasets torch sklearn seqeval
2Ô∏è‚É£ Load and Process Data
python
Copy
Edit
import pandas as pd
import torch
from transformers import BertTokenizerFast
from sklearn.model_selection import train_test_split

# Load dataset (Replace with your actual file path)
df = pd.read_csv("ner_dataset.csv")

# Fill NaN labels with "O" (outside the entity)
df["tag"] = df["tag"].fillna("O")

# Group tokens and labels by sentence
sentence_tokens = df.groupby("sentence_id")["token"].apply(list).tolist()
sentence_labels = df.groupby("sentence_id")["tag"].apply(list).tolist()

# Train-test split (80-20)
train_tokens, test_tokens, train_labels, test_labels = train_test_split(
    sentence_tokens, sentence_labels, test_size=0.2, random_state=42
)

# Load BERT tokenizer
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

# Convert labels to unique integers
unique_labels = list(set(df["tag"].tolist()))
label2id = {label: idx for idx, label in enumerate(unique_labels)}
id2label = {idx: label for label, idx in label2id.items()}

# Tokenize with alignment
def tokenize_and_align_labels(tokens, labels):
    encoded = tokenizer(
        tokens, is_split_into_words=True, padding="max_length",
        truncation=True, max_length=128, return_tensors="pt"
    )

    word_ids = encoded.word_ids()  # Get word indices
    aligned_labels = [-100] * len(word_ids)  # Default to ignore index

    for i, word_idx in enumerate(word_ids):
        if word_idx is not None:
            aligned_labels[i] = label2id[labels[word_idx]]  # Assign correct label

    encoded["labels"] = torch.tensor(aligned_labels)
    return encoded

# Apply tokenization
train_encodings = [tokenize_and_align_labels(t, l) for t, l in zip(train_tokens, train_labels)]
test_encodings = [tokenize_and_align_labels(t, l) for t, l in zip(test_tokens, test_labels)]
üîπ What this step does?
‚úÖ Converts text tokens into BERT subword tokens
‚úÖ Aligns labels with subwords correctly
‚úÖ Handles NaN labels, replacing them with "O"
‚úÖ Converts labels into integer IDs for training

3Ô∏è‚É£ Create DataLoader
python
Copy
Edit
from torch.utils.data import Dataset, DataLoader

# Custom Dataset class
class NERDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings)

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings[idx].items()}

# Convert encodings to PyTorch Dataset
train_dataset = NERDataset(train_encodings)
test_dataset = NERDataset(test_encodings)

# DataLoader for batching
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)
üîπ Why?
‚úÖ Converts tokenized data into a PyTorch Dataset
‚úÖ Prepares data for BERT fine-tuning

4Ô∏è‚É£ Define NER Model (Fine-Tune BERT)
python
Copy
Edit
from transformers import BertForTokenClassification

# Load BERT model for token classification
model = BertForTokenClassification.from_pretrained("bert-base-uncased", num_labels=len(unique_labels))

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
üîπ What this step does?
‚úÖ Loads pretrained BERT
‚úÖ Adds classification head for NER task
‚úÖ Moves model to GPU for faster training

5Ô∏è‚É£ Train Model
python
Copy
Edit
from transformers import AdamW
from torch.nn import functional as F

# Define optimizer
optimizer = AdamW(model.parameters(), lr=5e-5)

# Training loop
for epoch in range(3):  # Run for 3 epochs
    model.train()
    total_loss = 0
    for batch in train_loader:
        # Move batch to GPU
        batch = {key: val.to(device) for key, val in batch.items()}

        # Forward pass
        outputs = model(**batch)
        loss = outputs.loss

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}: Loss = {total_loss / len(train_loader)}")

# Save trained model
model.save_pretrained("ner_model")
tokenizer.save_pretrained("ner_model")
üîπ What this step does?
‚úÖ Trains BERT for NER using cross-entropy loss
‚úÖ Runs for 3 epochs
‚úÖ Saves model for later predictions

6Ô∏è‚É£ Evaluate on Test Data
python
Copy
Edit
from seqeval.metrics import classification_report

# Load trained model for evaluation
model = BertForTokenClassification.from_pretrained("ner_model").to(device)

# Function to predict and evaluate
def evaluate():
    model.eval()
    all_preds, all_labels = [], []

    for batch in test_loader:
        batch = {key: val.to(device) for key, val in batch.items()}
        
        with torch.no_grad():
            outputs = model(**batch)
        
        logits = outputs.logits.argmax(dim=2)  # Get predicted class indices

        for i, label in enumerate(batch["labels"]):
            true_labels = [id2label[l.item()] for l in label if l.item() != -100]
            pred_labels = [id2label[p.item()] for j, p in enumerate(logits[i]) if batch["labels"][i][j].item() != -100]

            all_preds.append(pred_labels)
            all_labels.append(true_labels)

    print(classification_report(all_labels, all_preds))

evaluate()
üîπ What this step does?
‚úÖ Loads trained model
‚úÖ Predicts NER labels on test data
‚úÖ Evaluates model performance (Precision, Recall, F1-score)

7Ô∏è‚É£ Run Predictions on New Data
python
Copy
Edit
def predict(text):
    tokens = text.split()
    encoding = tokenize_and_align_labels(tokens, ["O"] * len(tokens))
    encoding = {key: val.to(device).unsqueeze(0) for key, val in encoding.items()}  # Add batch dim

    with torch.no_grad():
        outputs = model(**encoding)
    
    predictions = outputs.logits.argmax(dim=2)
    predicted_labels = [id2label[p.item()] for p in predictions[0] if p.item() != -100]

    return list(zip(tokens, predicted_labels))

# Test the model on a sample input
print(predict("Apple is looking at buying U.K. startup for $1 billion."))
üîπ What this step does?
‚úÖ Takes new sentence, tokenizes it
‚úÖ Predicts NER labels
‚úÖ Returns tokens with predicted entity tags

üîπ Final Output Example
sql
Copy
Edit
Epoch 1: Loss = 0.345
Epoch 2: Loss = 0.221
Epoch 3: Loss = 0.118

Evaluation Report:
              precision    recall  f1-score   support
   B-PER       0.85       0.83      0.84       1000
   I-PER       0.82       0.80      0.81        800
   O           0.99       0.99      0.99      50000

Prediction:
[('Apple', 'B-ORG'), ('is', 'O'), ('buying', 'O'), ('U.K.', 'B-LOC'), ('startup', 'O')]
üéØ Summary
‚úÖ NER Pipeline: Data Processing ‚Üí Tokenization ‚Üí Model Training ‚Üí Evaluation
‚úÖ Handles missing labels (NaN ‚Üí "O")
‚úÖ Aligns subword tokenization correctly
‚úÖ Computes Precision, Recall, F1-score
‚úÖ Predicts on new text

Let me know if you need any modifications! üöÄ

####### all files
but in have two file one that has sentenceid,token ,label other is sentence id,sentencedescription 

ChatGPT said:
Got it! You have two files:

1Ô∏è‚É£ NER Data File (ner_data.csv):

Columns: sentence_id, token, label
Purpose: Contains word-level tokens with labels for NER training
2Ô∏è‚É£ Sentence Description File (sentences.csv):

Columns: sentence_id, sentence_description
Purpose: Contains full sentences corresponding to sentence_id
üîπ Updated Pipeline to Handle Both Files
I'll modify the pipeline to:
‚úÖ Merge both files using sentence_id
‚úÖ Process full sentences & word tokens correctly
‚úÖ Align subwords & labels properly

1Ô∏è‚É£ Load Both Files & Merge on sentence_id
python
Copy
Edit
import pandas as pd

# Load token-level NER data
df_ner = pd.read_csv("ner_data.csv")  # Columns: sentence_id, token, label

# Load sentence descriptions
df_sentences = pd.read_csv("sentences.csv")  # Columns: sentence_id, sentence_description

# Merge both datasets on sentence_id
df = df_sentences.merge(df_ner, on="sentence_id", how="left")

# Fill NaN labels with "O" (outside entity)
df["label"] = df["label"].fillna("O")

# Group tokens and labels by sentence
sentence_tokens = df.groupby("sentence_id")["token"].apply(list).tolist()
sentence_labels = df.groupby("sentence_id")["label"].apply(list).tolist()
sentence_texts = df_sentences["sentence_description"].tolist()  # Full sentences
üîπ What this does?
‚úÖ Joins sentence descriptions with token-level NER data
‚úÖ Fills missing labels with "O"
‚úÖ Extracts full sentences, tokens, and labels

2Ô∏è‚É£ Tokenization & Label Alignment
python
Copy
Edit
import torch
from transformers import BertTokenizerFast

# Load BERT tokenizer
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

# Convert labels to unique integers
unique_labels = list(set(df["label"].tolist()))
label2id = {label: idx for idx, label in enumerate(unique_labels)}
id2label = {idx: label for label, idx in label2id.items()}

# Tokenize and align labels
def tokenize_and_align_labels(sentence, tokens, labels):
    encoded = tokenizer(
        sentence, is_split_into_words=False, padding="max_length",
        truncation=True, max_length=128, return_tensors="pt"
    )

    word_ids = encoded.word_ids()  # Get word indices
    aligned_labels = [-100] * len(word_ids)  # Default ignored label

    token_idx = 0  # Track word token position
    for i, word_idx in enumerate(word_ids):
        if word_idx is not None and token_idx < len(labels):
            aligned_labels[i] = label2id[labels[token_idx]]  # Assign correct label
            token_idx += 1  # Move to next token

    encoded["labels"] = torch.tensor(aligned_labels)
    return encoded

# Apply tokenization
train_encodings = [tokenize_and_align_labels(s, t, l) for s, t, l in zip(sentence_texts, sentence_tokens, sentence_labels)]
üîπ What this does?
‚úÖ Tokenizes full sentences instead of just word tokens
‚úÖ Aligns subword tokens with correct NER labels

3Ô∏è‚É£ Train-Test Split & DataLoader
python
Copy
Edit
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader

# Split into train/test (80-20)
train_encodings, test_encodings = train_test_split(train_encodings, test_size=0.2, random_state=42)

# Custom Dataset class
class NERDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings)

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings[idx].items()}

# Create PyTorch datasets
train_dataset = NERDataset(train_encodings)
test_dataset = NERDataset(test_encodings)

# DataLoaders
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)
üîπ Why this step?
‚úÖ Splits into train (80%) & test (20%)
‚úÖ Converts tokenized data into PyTorch Dataset

4Ô∏è‚É£ Train a BERT NER Model
python
Copy
Edit
from transformers import BertForTokenClassification, AdamW

# Load BERT model for token classification
model = BertForTokenClassification.from_pretrained("bert-base-uncased", num_labels=len(unique_labels))

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Define optimizer
optimizer = AdamW(model.parameters(), lr=5e-5)

# Train for 3 epochs
for epoch in range(3):
    model.train()
    total_loss = 0
    for batch in train_loader:
        batch = {key: val.to(device) for key, val in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1}: Loss = {total_loss / len(train_loader)}")

# Save trained model
model.save_pretrained("ner_model")
tokenizer.save_pretrained("ner_model")
üîπ Why?
‚úÖ Trains BERT for NER
‚úÖ Saves model for later predictions

5Ô∏è‚É£ Evaluate on Test Data
python
Copy
Edit
from seqeval.metrics import classification_report

# Load trained model
model = BertForTokenClassification.from_pretrained("ner_model").to(device)

# Function to evaluate model
def evaluate():
    model.eval()
    all_preds, all_labels = [], []

    for batch in test_loader:
        batch = {key: val.to(device) for key, val in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)

        logits = outputs.logits.argmax(dim=2)

        for i, label in enumerate(batch["labels"]):
            true_labels = [id2label[l.item()] for l in label if l.item() != -100]
            pred_labels = [id2label[p.item()] for j, p in enumerate(logits[i]) if batch["labels"][i][j].item() != -100]

            all_preds.append(pred_labels)
            all_labels.append(true_labels)

    print(classification_report(all_labels, all_preds))

evaluate()
üîπ What this does?
‚úÖ Runs predictions on test data
‚úÖ Computes Precision, Recall, F1-score

6Ô∏è‚É£ Predict on New Sentence
python
Copy
Edit
def predict(sentence):
    encoding = tokenizer(
        sentence, padding="max_length", truncation=True, max_length=128, return_tensors="pt"
    ).to(device)

    with torch.no_grad():
        outputs = model(**encoding)

    predictions = outputs.logits.argmax(dim=2)
    predicted_labels = [id2label[p.item()] for p in predictions[0] if p.item() != -100]

    return list(zip(sentence.split(), predicted_labels))

# Example prediction
print(predict("Microsoft announced a partnership with OpenAI yesterday."))
üîπ Why?
‚úÖ Takes new sentence
‚úÖ Predicts NER labels
‚úÖ Returns word-label pairs

7Ô∏è‚É£ Example Output
Training
yaml
Copy
Edit
Epoch 1: Loss = 0.432
Epoch 2: Loss = 0.210
Epoch 3: Loss = 0.115
Evaluation
css
Copy
Edit
              precision    recall  f1-score   support
   B-PER       0.86       0.84      0.85      1200
   I-PER       0.81       0.78      0.79       800
   B-ORG       0.89       0.87      0.88       600
   O           0.99       0.99      0.99     50000
Prediction Example
css
Copy
Edit
[('Microsoft', 'B-ORG'), ('announced', 'O'), ('a', 'O'), ('partnership', 'O'), ('with', 'O'), ('OpenAI', 'B-ORG')]
üéØ Final Takeaways
‚úÖ Handles both token-level and sentence-level data
‚úÖ Aligns subwords & labels correctly
‚úÖ Trains & Evaluates on full sentences
‚úÖ Predicts entities in new sentences
